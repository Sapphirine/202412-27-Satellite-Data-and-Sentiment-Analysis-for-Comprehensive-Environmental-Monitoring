{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I - Packages Import\n"
      ],
      "metadata": {
        "id": "yCy9LTrEeBVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, pipeline\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n"
      ],
      "metadata": {
        "id": "cTBgAeVJb8P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "II - Data Verification / Preprocessing\n",
        "\n",
        "This code verifies data format and time distribution of the articles"
      ],
      "metadata": {
        "id": "E8c9TNCAeJC2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xILquOltl7Q_"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('guardian_environment_news.csv')\n",
        "\n",
        "# Inspect the first few rows of the dataset\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ny2o30Sl7RA"
      },
      "outputs": [],
      "source": [
        "# Ensure 'Date Published' is in datetime format\n",
        "df['Date Published'] = pd.to_datetime(df['Date Published'], errors='coerce')\n",
        "\n",
        "# Check for invalid dates\n",
        "if df['Date Published'].isnull().any():\n",
        "    print(\"Warning: Some dates could not be parsed and have been set to NaT.\")\n",
        "# Select year 2023\n",
        "df = df[df['Date Published'] >= '2023-01-01']\n",
        "df = df[df['Date Published'] < '2024-01-01']\n",
        "# Group by month and year\n",
        "df['Year-Month'] = df['Date Published'].dt.to_period('M')  # Create 'Year-Month' column\n",
        "monthly_counts = df['Year-Month'].value_counts().sort_index()\n",
        "\n",
        "# Plot the aggregated distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(monthly_counts.index.astype(str), monthly_counts.values, color='skyblue')\n",
        "plt.title(\"Monthly Article Publication Distribution\", fontsize=16)\n",
        "plt.xlabel(\"Month\", fontsize=14)\n",
        "plt.ylabel(\"Number of Articles\", fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(alpha=0.5, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "III - Features Extraction\n",
        "\n",
        "This code uses different NLP pipeline to extract the different relevant features from the text."
      ],
      "metadata": {
        "id": "_yJBRBNDeOIg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23ITcr4he0kQ"
      },
      "outputs": [],
      "source": [
        "# Initialize a named entity recognition (NER) pipeline using a pre-trained BERT model fine-tuned on CoNLL-2003 English data.\n",
        "ner_pipeline = pipeline(\n",
        "    'ner',  # Task: Named Entity Recognition\n",
        "    model='dbmdz/bert-large-cased-finetuned-conll03-english',  # Pre-trained model\n",
        "    tokenizer='dbmdz/bert-large-cased-finetuned-conll03-english'  # Corresponding tokenizer\n",
        ")\n",
        "\n",
        "# Function to extract named entities from a given text using the NER pipeline.\n",
        "def get_named_entities(text):\n",
        "    entities = ner_pipeline(text)  # Perform NER on the input text\n",
        "    return entities\n",
        "\n",
        "# Apply NER to each row of the DataFrame and store the results in a new column 'NER Location'.\n",
        "df['NER Location'] = df.apply(\n",
        "    lambda row: (\n",
        "        print(f\"Processing index: {row.name}\") or  # Print the index being processed for tracking\n",
        "        (get_named_entities(row['Article Text']) if pd.notna(row['Article Text']) else None)  # Perform NER if the text is not NaN\n",
        "    ),\n",
        "    axis=1  # Apply the function row-wise\n",
        ")\n",
        "\n",
        "# Save the updated DataFrame with the NER results to a CSV file\n",
        "df.to_csv('sample_NER.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoJMyDc-l7RA"
      },
      "outputs": [],
      "source": [
        "# Initialize a sentiment analysis pipeline using a pre-trained DistilBERT model fine-tuned on SST-2 English data.\n",
        "sent_classifier = pipeline(\n",
        "    'sentiment-analysis',  # Task: Sentiment Analysis\n",
        "    model='distilbert-base-uncased-finetuned-sst-2-english',  # Pre-trained model\n",
        "    truncation=True  # Truncate input text to fit the model's input size\n",
        ")\n",
        "\n",
        "# Function to compute the sentiment score for a given text.\n",
        "def get_sentiment_score(text):\n",
        "    result = sent_classifier(text)  # Perform sentiment analysis on the input text\n",
        "    score = result[0]['score']  # Extract the sentiment score\n",
        "    return score\n",
        "\n",
        "# Apply sentiment analysis to each row of the DataFrame and store the scores in a new column 'Sentiment Score'.\n",
        "df['Sentiment Score'] = df.apply(\n",
        "    lambda row: (\n",
        "        print(f\"Processing index: {row.name}\") or  # Print the index being processed for tracking\n",
        "        (get_sentiment_score(row['Article Text']) if pd.notna(row['Article Text']) else None)  # Compute sentiment if the text is not NaN\n",
        "    ),\n",
        "    axis=1  # Apply the function row-wise\n",
        ")\n",
        "\n",
        "# Save the updated DataFrame with sentiment scores to a CSV file named 'df_NER+sent.csv'.\n",
        "df.to_csv('df_NER+sent.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3bjHw4Nrq-q"
      },
      "outputs": [],
      "source": [
        "# Initialize an emotion classification pipeline using a pre-trained DistilRoBERTa model for emotion detection.\n",
        "emo_classifier = pipeline(\n",
        "    'text-classification',  # Task: Text Classification\n",
        "    model='j-hartmann/emotion-english-distilroberta-base',  # Pre-trained model for emotion classification\n",
        "    tokenizer='j-hartmann/emotion-english-distilroberta-base',  # Corresponding tokenizer\n",
        "    truncation=True  # Truncate input text to fit the model's input size\n",
        ")\n",
        "\n",
        "# Function to compute emotion scores for a given text.\n",
        "def get_emotion_score(article_text):\n",
        "    result = emo_classifier(article_text)  # Perform emotion classification on the input text\n",
        "    return result\n",
        "\n",
        "# Apply emotion classification to each row of the DataFrame and store the results in a new column 'Emotion'.\n",
        "df['Emotion'] = df.apply(\n",
        "    lambda row: (\n",
        "        print(f\"Processing index: {row.name}\") or  # Print the index being processed for tracking\n",
        "        (get_emotion_score(row['Article Text']) if pd.notna(row['Article Text']) else None)  # Compute emotion if the text is not NaN\n",
        "    ),\n",
        "    axis=1  # Apply the function row-wise\n",
        ")\n",
        "\n",
        "# Function to extract the highest scoring emotion from the emotion data.\n",
        "def extract_highest_emotion(emotion_data):\n",
        "    try:\n",
        "        # Convert the string representation of emotion data into a list of dictionaries.\n",
        "        emotions = ast.literal_eval(emotion_data)\n",
        "\n",
        "        # Find the emotion with the highest score and return its label.\n",
        "        highest_emotion = max(emotions, key=lambda x: x['score'])['label']\n",
        "        return highest_emotion\n",
        "    except (ValueError, SyntaxError, TypeError) as e:\n",
        "        # If there is an error (e.g., malformed string), return 'neutral' as the default.\n",
        "        return 'neutral'\n",
        "\n",
        "# Apply the function to the 'Emotion' column.\n",
        "df['Emotion_Label'] = df['Emotion'].apply(extract_highest_emotion)\n",
        "\n",
        "# Save the updated DataFrame with emotion classification results to a CSV file.\n",
        "df.to_csv('df_NER+sent+emo.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of country names used to filter the NER results.\n",
        "country_list = [\n",
        "    'Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola', 'Antigua and Barbuda', 'Argentina', 'Armenia',\n",
        "    'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium',\n",
        "    'Belize', 'Benin', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Brunei', 'Bulgaria',\n",
        "    'Burkina Faso', 'Burundi', 'Cabo Verde', 'Cambodia', 'Cameroon', 'Canada', 'Central African Republic', 'Chad',\n",
        "    'Chile', 'China', 'Colombia', 'Comoros', 'Congo (Congo-Brazzaville)', 'Congo (Congo-Kinshasa)', 'Costa Rica',\n",
        "    'Croatia', 'Cuba', 'Cyprus', 'Czechia (Czech Republic)', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic',\n",
        "    'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Eswatini (fmr. \"Swaziland\")', 'England',\n",
        "    'Ethiopia', 'Fiji', 'Finland', 'France', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Greece', 'Grenada', 'Great Britain',\n",
        "    'Guatemala', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia',\n",
        "    'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati',\n",
        "    'Korea, North', 'Korea, South', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya',\n",
        "    'Liechtenstein', 'Lithuania', 'Luxembourg', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands',\n",
        "    'Mauritania', 'Mauritius', 'Mexico', 'Micronesia', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique',\n",
        "    'Myanmar (formerly Burma)', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria',\n",
        "    'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines',\n",
        "    'Poland', 'Portugal', 'Qatar', 'Romania', 'Russia', 'Rwanda', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Vincent and the Grenadines',\n",
        "    'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore',\n",
        "    'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname',\n",
        "    'Sweden', 'Switzerland', 'Syria', 'Taiwan', 'Tajikistan', 'Tanzania', 'Thailand', 'Timor-Leste', 'Togo', 'Tonga', 'Trinidad and Tobago',\n",
        "    'Tunisia', 'Turkey', 'Turkmenistan', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom','United-Kingdom','England', 'UK', 'United States of America', 'United States', 'USA',\n",
        "    'Uruguay', 'Uzbekistan', 'Vanuatu', 'Vatican City', 'Venezuela', 'Vietnam', 'Yemen', 'Zambia', 'Zimbabwe'\n",
        "]\n",
        "\n",
        "# Function to extract countries from NER data based on the country list.\n",
        "def extract_countries(ner_data, country_list):\n",
        "    # Extract countries from NER results where the entity is identified as 'I-LOC' and the word matches a country in the country list.\n",
        "    countries = [\n",
        "        item['word'] for item in ner_data if item['entity'] == 'I-LOC' and item['word'] in country_list\n",
        "    ]\n",
        "    return list(set(countries))  # Use `set` to avoid duplicates and return unique country names\n",
        "\n",
        "# Apply the function to the 'NER Location' column and store the extracted countries in a new column 'countries'.\n",
        "df['countries'] = df['NER Location'].apply(lambda x: extract_countries(x, country_list))\n",
        "\n",
        "# Save the updated DataFrame with extracted countries to a CSV file.\n",
        "df.to_csv('df_NER+sent+emo+countries.csv', index=False)"
      ],
      "metadata": {
        "id": "4a9fOC0XbExP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('final_df.csv', index=False)"
      ],
      "metadata": {
        "id": "CXlpDRa5bjWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IV - Features Post-Processing\n",
        "\n",
        " This code splits the dataframe with month interval and full year interval in order to create a result dataframe containing the necessary data to plot the maps.\n",
        "   "
      ],
      "metadata": {
        "id": "s3SW82o7eVkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#df = pd.read_csv('final_df.csv')\n",
        "\n",
        "# Filter data per publication date\n",
        "\n",
        "df1 = df[df['Date Published'] >= '2023-01-01']\n",
        "df1 = df1[df1['Date Published'] < '2023-02-01']\n",
        "\n",
        "df2 = df[df['Date Published'] >= '2023-02-01']\n",
        "df2 = df2[df2['Date Published'] < '2023-03-01']\n",
        "\n",
        "df3 = df[df['Date Published'] >= '2023-03-01']\n",
        "df3 = df3[df3['Date Published'] < '2023-04-01']\n",
        "\n",
        "df4 = df[df['Date Published'] >= '2023-04-01']\n",
        "df4 = df4[df4['Date Published'] < '2023-05-01']\n",
        "\n",
        "df5 = df[df['Date Published'] >= '2023-05-01']\n",
        "df5 = df5[df5['Date Published'] < '2023-06-01']\n",
        "\n",
        "df6 = df[df['Date Published'] >= '2023-06-01']\n",
        "df6 = df6[df6['Date Published'] < '2023-07-01']\n",
        "\n",
        "df7 = df[df['Date Published'] >= '2023-07-01']\n",
        "df7 = df7[df7['Date Published'] < '2023-08-01']\n",
        "\n",
        "df8 = df[df['Date Published'] >= '2023-08-01']\n",
        "df8 = df8[df8['Date Published'] < '2023-09-01']\n",
        "\n",
        "df9 = df[df['Date Published'] >= '2023-09-01']\n",
        "df9 = df9[df9['Date Published'] < '2023-10-01']\n",
        "\n",
        "df10 = df[df['Date Published'] >= '2023-10-01']\n",
        "df10 = df10[df10['Date Published'] < '2023-11-01']\n",
        "\n",
        "df11 = df[df['Date Published'] >= '2023-11-01']\n",
        "df11 = df11[df11['Date Published'] < '2023-12-01']\n",
        "\n",
        "df12 = df[df['Date Published'] >= '2023-12-01']\n",
        "df12 = df12[df12['Date Published'] < '2024-01-01']\n",
        "\n",
        "df13 = df[df['Date Published'] >= '2023-01-01']\n",
        "df13 = df13[df13['Date Published'] < '2024-01-01']\n",
        "print(df13.shape)\n",
        "\n",
        "# Create a list of dataframes for each time period\n",
        "dfs= [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13]\n"
      ],
      "metadata": {
        "id": "dOG8Y4DJbj-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "def final_processing(dfs):\n",
        "    for idx, df in enumerate(dfs):\n",
        "        # Sample DataFrame (replace this with your actual data)\n",
        "        df2 = df[['countries', 'Sentiment Score', 'Emotion_Label', 'topics']]  # Ensure 'Emotion_Label' and 'topics' columns exist\n",
        "\n",
        "        # Step 1: Convert 'countries' column from string to list and remove redundancy\n",
        "        df2['countries'] = df2['countries'].apply(lambda x: list(set(ast.literal_eval(x))) if isinstance(x, str) else x)\n",
        "\n",
        "        # Step 2: Explode the 'countries' column into separate rows\n",
        "        df2_exploded = df2.explode('countries').reset_index(drop=True)\n",
        "\n",
        "        # -------------------- Topics Counting --------------------\n",
        "\n",
        "        # Define the topics_to_keep list (replace with your actual topics)\n",
        "        topics_to_keep = ['water', 'forest', 'energy', 'pollution', 'biodiversity']  # Replace with your topics\n",
        "\n",
        "        # Step 3: Explode the 'topics' column into separate rows (like 'countries')\n",
        "        df2_exploded['topics'] = df2_exploded['topics'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "        df2_exploded = df2_exploded.explode('topics').reset_index(drop=True)\n",
        "\n",
        "        # Step 4: Create a column for each topic in topics_to_keep with a count of how many times each topic is mentioned per country\n",
        "        for topic in topics_to_keep:\n",
        "            df2_exploded[topic] = df2_exploded['topics'].apply(lambda x: 1 if x == topic else 0)\n",
        "\n",
        "        # Step 5: Group by 'countries' and calculate the mean sentiment score\n",
        "        result_topics = df2_exploded.groupby('countries', as_index=False)['Sentiment Score'].mean()\n",
        "\n",
        "        # Step 6: Add the topic counts to the result_topics DataFrame\n",
        "        result_topics[topics_to_keep] = df2_exploded.groupby('countries')[topics_to_keep].sum().reset_index(drop=True)\n",
        "\n",
        "        # -------------------- Emotion Counting --------------------\n",
        "\n",
        "        # Define the unique emotions (replace with actual emotions extracted from 'Emotion_Label' column)\n",
        "        unique_emotions = ['disgust', 'anger', 'surprise', 'neutral', 'joy', 'sadness', 'fear']  # Replace with actual list of unique emotions\n",
        "\n",
        "        # Step 7: Count the occurrences of each emotion for each country\n",
        "        emotion_counts = df2_exploded.groupby(['countries', 'Emotion_Label']).size().unstack(fill_value=0)\n",
        "\n",
        "        # Step 8: Find the emotion with the highest count for each country\n",
        "        emotion_counts['Dominant_Emotion'] = emotion_counts.idxmax(axis=1)\n",
        "\n",
        "        # Step 9: Create a DataFrame with the dominant emotion for each country\n",
        "        result_emotions = emotion_counts[['Dominant_Emotion']].reset_index()\n",
        "\n",
        "        # -------------------- Add Country Count --------------------\n",
        "\n",
        "        country_count = df2.explode('countries')['countries'].value_counts().reset_index()\n",
        "        country_count.columns = ['countries', 'Country_Count']\n",
        "\n",
        "        # Merge the country count into result_emotions\n",
        "        result_emotions = pd.merge(result_emotions, country_count, on='countries', how='left')\n",
        "\n",
        "        # -------------------- Merge the Results --------------------\n",
        "\n",
        "        # Merge the topic counts and emotion counts into one final DataFrame\n",
        "        final_result = pd.merge(result_topics, result_emotions, on='countries', suffixes=('_topic', '_emotion'))\n",
        "        # Save result dataframe as csv\n",
        "        final_result.to_csv(f'final_result_{idx}.csv', index=False)\n",
        "\n",
        "# Call function\n",
        "final_processing(dfs)\n"
      ],
      "metadata": {
        "id": "eA7VklERbmku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "V - Map Plotting\n",
        "\n",
        "This code use the result dataframes to plot the maps for each feature and each time period."
      ],
      "metadata": {
        "id": "2bi2duqReaXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U kaleido"
      ],
      "metadata": {
        "id": "1mQWVUJTbsn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output_folder = \"last_maps\"\n",
        "\n",
        "# Create output folder\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "def plot(idx):\n",
        "    # Import specific time period result dataframe\n",
        "    results = pd.read_csv(f'final_result_{idx}.csv')\n",
        "\n",
        "    fig = px.choropleth(\n",
        "        results,  # Data source for the map\n",
        "        locations=\"countries\",  # Column in DataFrame with country names\n",
        "        locationmode='country names',  # Match locations based on country names\n",
        "        color=\"water\",  # Feature to use for color intensity on the map\n",
        "        color_continuous_scale=px.colors.sequential.Plasma  # Color scale for the map\n",
        "    )\n",
        "    # Save the generated choropleth map as an image file.\n",
        "    fig.write_image(os.path.join(output_folder, f\"water_{idx}.png\"))\n",
        "\n",
        "    fig = px.choropleth(results, locations=\"countries\",\n",
        "                        locationmode='country names',\n",
        "                        color=\"Sentiment Score\",\n",
        "                        color_continuous_scale=px.colors.sequential.Plasma)\n",
        "\n",
        "    fig.write_image(os.path.join(output_folder, f\"sent_{idx}.png\"))\n",
        "\n",
        "\n",
        "    fig = px.choropleth(results, locations=\"countries\",\n",
        "                        locationmode='country names',\n",
        "                        color=\"forest\",\n",
        "                        color_continuous_scale=px.colors.sequential.Plasma)\n",
        "\n",
        "    fig.write_image(os.path.join(output_folder, f\"forest_{idx}.png\"))\n",
        "\n",
        "\n",
        "    fig = px.choropleth(results, locations=\"countries\",\n",
        "                        locationmode='country names',\n",
        "                        color=\"energy\",\n",
        "                        color_continuous_scale=px.colors.sequential.Plasma)\n",
        "\n",
        "    fig.write_image(os.path.join(output_folder, f\"energy_{idx}.png\"))\n",
        "\n",
        "\n",
        "    fig = px.choropleth(results, locations=\"countries\",\n",
        "                        locationmode='country names',\n",
        "                        color=\"biodiversity\",\n",
        "                        color_continuous_scale=px.colors.sequential.Plasma)\n",
        "\n",
        "    fig.write_image(os.path.join(output_folder, f\"biodiversity_{idx}.png\"))\n",
        "\n",
        "    fig = px.choropleth(results, locations=\"countries\",\n",
        "                        locationmode='country names',\n",
        "                        color=\"Dominant_Emotion\",\n",
        "                        color_continuous_scale=px.colors.sequential.Plasma)\n",
        "\n",
        "    fig.write_image(os.path.join(output_folder, f\"emo_{idx}.png\"))\n",
        "\n",
        "    fig = px.choropleth(results, locations=\"countries\",\n",
        "                        locationmode='country names',\n",
        "                        color=\"Country_Count\",\n",
        "                        color_continuous_scale=px.colors.sequential.Plasma)\n",
        "\n",
        "    fig.write_image(os.path.join(output_folder, f\"count_{idx}.png\"))\n",
        "\n",
        "#This loop will use the plot function on each month data and on full year data\n",
        "for i in range(13):\n",
        "  plot(i)"
      ],
      "metadata": {
        "id": "EuSKxazBcWIo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}